import json
import logging
import time
import re
import os
from pathlib import Path
from typing import Dict, List, Optional

import streamlit as st
import numpy as np
import cv2
from PIL import Image
from selenium.webdriver.common.by import By
import undetected_chromedriver as uc
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)


class LocalBrowserAgent:
    """–õ–æ–∫–∞–ª—å–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä–Ω—ã–π –∞–≥–µ–Ω—Ç –Ω–∞ Selenium/undetected-chromedriver"""

    def __init__(self, headless: bool = True):
        self.headless = headless
        self.driver = None
        self.screenshots_dir = Path("screenshots")
        self.text_dir = Path("extracted_text")
        self.screenshots_dir.mkdir(exist_ok=True)
        self.text_dir.mkdir(exist_ok=True)

    def open(self):
        logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä–Ω—ã–π –∞–≥–µ–Ω—Ç (Selenium)...")
        options = Options()
        if self.headless:
            options.add_argument('--headless=new')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        options.add_argument('--lang=ru-RU')
        options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        self.driver = uc.Chrome(options=options)
        logger.info("‚úÖ –ë—Ä–∞—É–∑–µ—Ä–Ω—ã–π –∞–≥–µ–Ω—Ç –∑–∞–ø—É—â–µ–Ω")

    def close(self):
        if self.driver:
            self.driver.quit()
        logger.info("‚õî –ë—Ä–∞—É–∑–µ—Ä–Ω—ã–π –∞–≥–µ–Ω—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    def search_address_in_yandex(self, address: str) -> Dict:
        logger.info(f"üîç –ò—â–µ–º –∞–¥—Ä–µ—Å –≤ –±—Ä–∞—É–∑–µ—Ä–µ: {address}")
        timestamp = int(time.time())
        safe_address = re.sub(r'[^\w\s-]', '', address).replace(' ', '_')[:30]
        safe_address = self._transliterate_russian(safe_address)
        screenshot_filename = f"search_{timestamp}_{safe_address}.png"
        text_filename = f"text_{timestamp}_{safe_address}.txt"
        screenshot_path = self.screenshots_dir / screenshot_filename
        text_path = self.text_dir / text_filename
        try:
            from urllib.parse import quote_plus
            search_url = f"https://yandex.ru/search/?text={quote_plus(address)}"
            logger.info(f"üåê –ü–µ—Ä–µ—Ö–æ–¥–∏–º –ø–æ –ø—Ä—è–º–æ–º—É –ø–æ–∏—Å–∫–æ–≤–æ–º—É URL: {search_url}")
            # –°–∫—Ä–∏–Ω—à–æ—Ç –î–û –ø–æ–∏—Å–∫–∞ (–≥–ª–∞–≤–Ω–∞—è)
            try:
                self.driver.get("https://yandex.ru")
                time.sleep(2)
                self.driver.save_screenshot(str(screenshot_path))
                logger.info(f"‚úÖ –°–∫—Ä–∏–Ω—à–æ—Ç –≥–ª–∞–≤–Ω–æ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {screenshot_path}")
            except Exception as e:
                logger.warning(
                    f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–¥–µ–ª–∞—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç –≥–ª–∞–≤–Ω–æ–π: {str(e)}")
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞
            self.driver.get(search_url)
            time.sleep(3)
            final_screenshot_path = self.screenshots_dir / \
                f"final_{screenshot_filename}"
            self.driver.save_screenshot(str(final_screenshot_path))
            logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫—Ä–∏–Ω—à–æ—Ç: {final_screenshot_path}")
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ DOM
            results = self._extract_search_results_from_dom()
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç —Å –ø–æ–º–æ—â—å—é –ò–ò
            ai_text_analysis = self._analyze_screenshot_with_ai(
                final_screenshot_path, address)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ —Ñ–∞–π–ª
            with open(text_path, 'w', encoding='utf-8') as f:
                f.write(f"–ê–¥—Ä–µ—Å: {address}\n")
                f.write(f"–í—Ä–µ–º—è: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"–°–∫—Ä–∏–Ω—à–æ—Ç: {final_screenshot_path}\n")
                f.write("="*50 + "\n")
                f.write(ai_text_analysis)
            logger.info(f"üíæ –¢–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {text_path}")
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            try:
                page_text = self.driver.find_element(By.TAG_NAME, 'body').text
                text_analysis = self._analyze_page_text(page_text, address)
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {str(e)}")
                text_analysis = "–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
            return {
                'address': address,
                'screenshot_path': str(final_screenshot_path),
                'text_file_path': str(text_path),
                'results': results,
                'ai_text_analysis': ai_text_analysis,
                'text_analysis': text_analysis,
                'page_title': self.driver.title,
                'page_url': self.driver.current_url,
                'success': True
            }
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –±—Ä–∞—É–∑–µ—Ä–µ: {str(e)}")
            try:
                error_screenshot = self.screenshots_dir / \
                    f"error_{screenshot_filename}"
                self.driver.save_screenshot(str(error_screenshot))
                logger.info(f"üì∏ –°–∫—Ä–∏–Ω—à–æ—Ç –æ—à–∏–±–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {error_screenshot}")
            except:
                pass
            return {
                'address': address,
                'error': str(e),
                'results': [],
                'ai_text_analysis': '–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ',
                'text_analysis': '–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–∫—Å—Ç–∞',
                'success': False
            }

    def _extract_search_results_from_dom(self) -> List[Dict]:
        results = []
        try:
            time.sleep(2)
            result_selectors = [
                '.serp-item', '.organic', 'li[data-cid]', '[data-log-node="serp-item"]',
                '.search-result', '.result', '.VanillaReact', '[data-bem*="serp-item"]'
            ]
            result_elements = []
            used_selector = None
            for selector in result_selectors:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    result_elements = elements
                    used_selector = selector
                    logger.info(
                        f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(elements)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {selector}")
                    break
            if not result_elements:
                logger.warning(
                    "‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã")
                all_links = self.driver.find_elements(
                    By.CSS_SELECTOR, 'a[href]')
                logger.info(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
                for i, link in enumerate(all_links[:10]):
                    try:
                        href = link.get_attribute('href')
                        text = link.text
                        if href and text and len(text.strip()) > 5:
                            results.append({
                                'rank': i + 1,
                                'title': text.strip()[:100],
                                'url': href,
                                'snippet': '',
                                'domain': self._extract_domain(href),
                                'result_type': self._determine_result_type(href, text, '')
                            })
                    except:
                        continue
                return results
            for idx, element in enumerate(result_elements[:10]):
                try:
                    result = self._extract_single_result(element, idx + 1)
                    if result:
                        results.append(result)
                except Exception as e:
                    logger.warning(
                        f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ {idx + 1}: {str(e)}")
                    continue
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ DOM: {str(e)}")
        logger.info(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ DOM")
        return results

    def _extract_single_result(self, element, rank: int) -> Optional[Dict]:
        try:
            title_selectors = [
                'h2 a', 'h3 a', '.organic__title-wrapper a',
                '.serp-item__title a', '.title a', 'a[data-log-node="title"]',
                '.VanillaReact a', '.link'
            ]
            title = ""
            url = ""
            for selector in title_selectors:
                try:
                    title_elem = element.find_element(
                        By.CSS_SELECTOR, selector)
                    if title_elem:
                        title = title_elem.text
                        url = title_elem.get_attribute('href') or ""
                        break
                except NoSuchElementException:
                    continue
            if not title:
                try:
                    link_elem = element.find_element(
                        By.CSS_SELECTOR, 'a[href]')
                    if link_elem:
                        title = link_elem.text or "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                        url = link_elem.get_attribute('href') or ""
                except NoSuchElementException:
                    pass
            snippet_selectors = [
                '.organic__text', '.serp-item__text', '.snippet',
                '.text-container', '.organic__content', '.content',
                '.VanillaReact .text'
            ]
            snippet = ""
            for selector in snippet_selectors:
                try:
                    snippet_elem = element.find_element(
                        By.CSS_SELECTOR, selector)
                    if snippet_elem:
                        snippet = snippet_elem.text
                        break
                except NoSuchElementException:
                    continue
            if not snippet:
                snippet = element.text
                snippet = snippet[:200] + \
                    "..." if len(snippet) > 200 else snippet
            domain = self._extract_domain(url)
            result_type = self._determine_result_type(url, title, snippet)
            if title or url:
                return {
                    'rank': rank,
                    'title': title.strip(),
                    'url': url,
                    'snippet': snippet.strip(),
                    'domain': domain,
                    'result_type': result_type
                }
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ {rank}: {str(e)}")
        return None

    def _extract_domain(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞ –∏–∑ URL"""
        if not url:
            return ""
        try:
            import re
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞ –±–µ–∑ urllib
            match = re.search(r'https?://([^/]+)', url)
            return match.group(1) if match else ""
        except:
            return ""

    def _determine_result_type(self, url: str, title: str, snippet: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–∏—Å–∫–∞"""
        url_lower = url.lower()
        title_lower = title.lower()
        snippet_lower = snippet.lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ URL
        if 'yandex.ru/maps' in url_lower or 'maps.yandex' in url_lower:
            return 'maps_yandex'
        elif '2gis.ru' in url_lower or '2gis.com' in url_lower:
            return 'maps_2gis'
        elif any(domain in url_lower for domain in ['avito.ru', 'cian.ru', 'domclick.ru', 'youla.ru']):
            return 'realestate'
        elif any(domain in url_lower for domain in ['rosreestr.ru', 'gosuslugi.ru', 'egrp365.ru']):
            return 'government'
        elif 'wikipedia' in url_lower:
            return 'encyclopedia'

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        map_keywords = ['–∫–∞—Ä—Ç', 'map', '–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç', '–º–∞—Ä—à—Ä—É—Ç', '–Ω–∞–≤–∏–≥–∞—Ü']
        if any(keyword in title_lower or keyword in snippet_lower for keyword in map_keywords):
            return 'maps_general'

        realestate_keywords = ['–Ω–µ–¥–≤–∏–∂–∏–º', '–∫–≤–∞—Ä—Ç–∏—Ä',
                               '–¥–æ–º', '–∞—Ä–µ–Ω–¥', '–ø—Ä–æ–¥–∞–∂', '—Ü–µ–Ω–∞']
        if any(keyword in title_lower or keyword in snippet_lower for keyword in realestate_keywords):
            return 'realestate'

        return 'website'

    def _analyze_screenshot_with_ai(self, screenshot_path: Path, address: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω–æ–π –ò–ò –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(
                f"ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç —Å –ø–æ–º–æ—â—å—é –ò–ò: {screenshot_path}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = cv2.imread(str(screenshot_path))
            if image is None:
                return "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"

            # OCR –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
            ocr_text = self._extract_text_with_ocr(image)

            # –ü—Ä–æ—Å—Ç–æ–π –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            height, width = image.shape[:2]

            analysis_parts = []
            analysis_parts.append(f"=== –ê–ù–ê–õ–ò–ó –°–ö–†–ò–ù–®–û–¢–ê ===")
            analysis_parts.append(f"–ê–¥—Ä–µ—Å: {address}")
            analysis_parts.append(f"–†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {width}x{height}")
            analysis_parts.append(
                f"–í—Ä–µ–º—è: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            analysis_parts.append("")

            # OCR —Ç–µ–∫—Å—Ç
            if ocr_text:
                analysis_parts.append("=== –ò–ó–í–õ–ï–ß–ï–ù–ù–´–ô –¢–ï–ö–°–¢ ===")
                analysis_parts.append(ocr_text)
                analysis_parts.append("")

                # –ê–Ω–∞–ª–∏–∑ OCR —Ç–µ–∫—Å—Ç–∞
                analysis_parts.append("=== –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–ê ===")

                # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
                text_lower = ocr_text.lower()

                if any(word in text_lower for word in ['–Ω–∞–π–¥–µ–Ω–æ', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–ø–æ–∫–∞–∑–∞–Ω–æ']):
                    analysis_parts.append("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞")

                    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    import re
                    numbers = re.findall(r'\d+', ocr_text)
                    if numbers:
                        analysis_parts.append(
                            f"üìä –ß–∏—Å–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ: {', '.join(numbers[:5])}")

                if any(word in text_lower for word in ['–∫–∞—Ä—Ç', 'map', '—è–Ω–¥–µ–∫—Å.–∫–∞—Ä—Ç—ã']):
                    analysis_parts.append("üó∫Ô∏è –ù–∞–π–¥–µ–Ω—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–∞—Ä—Ç")

                if any(word in text_lower for word in ['–∫–∞–ø—á–∞', 'captcha', '–ø—Ä–æ–≤–µ—Ä–∫–∞']):
                    analysis_parts.append("üõ°Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞!")

                if '–Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ' in text_lower:
                    analysis_parts.append("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")

                # –ò—â–µ–º –∞–¥—Ä–µ—Å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                address_words = address.lower().split()
                found_words = [word for word in address_words if len(
                    word) > 3 and word in text_lower]
                if found_words:
                    analysis_parts.append(
                        f"üìç –ù–∞–π–¥–µ–Ω—ã —Å–ª–æ–≤–∞ –∞–¥—Ä–µ—Å–∞: {', '.join(found_words)}")

            else:
                analysis_parts.append(
                    "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é OCR")

            # –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            analysis_parts.append("")
            analysis_parts.append("=== –ê–ù–ê–õ–ò–ó –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø ===")

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ HSV –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ü–≤–µ—Ç–æ–≤
            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

            # –ò—â–µ–º –∂–µ–ª—Ç—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è –Ø–Ω–¥–µ–∫—Å–∞)
            yellow_lower = np.array([15, 100, 100])
            yellow_upper = np.array([35, 255, 255])
            yellow_mask = cv2.inRange(hsv, yellow_lower, yellow_upper)
            yellow_pixels = cv2.countNonZero(yellow_mask)

            if yellow_pixels > 1000:
                analysis_parts.append(
                    "üü° –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —ç–ª–µ–º–µ–Ω—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –Ø–Ω–¥–µ–∫—Å–∞")

            # –ò—â–µ–º —Å–∏–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            blue_lower = np.array([100, 50, 50])
            blue_upper = np.array([130, 255, 255])
            blue_mask = cv2.inRange(hsv, blue_lower, blue_upper)
            blue_pixels = cv2.countNonZero(blue_mask)

            if blue_pixels > 500:
                analysis_parts.append("üîó –ù–∞–π–¥–µ–Ω—ã —ç–ª–µ–º–µ–Ω—Ç—ã, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —Å—Å—ã–ª–∫–∏")

            # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            contours, _ = cv2.findContours(
                edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            large_rectangles = sum(
                1 for contour in contours if cv2.contourArea(contour) > 5000)

            if large_rectangles > 5:
                analysis_parts.append(
                    f"üìã –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {large_rectangles} –∫—Ä—É–ø–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
            elif large_rectangles > 0:
                analysis_parts.append(
                    f"üìã –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {large_rectangles} –±–ª–æ–∫–æ–≤")
            else:
                analysis_parts.append("‚ö†Ô∏è –ú–∞–ª–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–ø—á—É
            if self._detect_captcha_in_image(gray):
                analysis_parts.append("üõ°Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –í–æ–∑–º–æ–∂–Ω–∞ –∫–∞–ø—á–∞!")

            return "\n".join(analysis_parts)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ò–ò –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
            return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}"

    def _detect_captcha_in_image(self, gray_image) -> bool:
        """–ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞–ø—á–∏ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
        try:
            # –ò—â–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è –∫–∞–ø—á–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            edges = cv2.Canny(gray_image, 100, 200)
            edge_density = cv2.countNonZero(
                edges) / (gray_image.shape[0] * gray_image.shape[1])

            # –ò—Å–∫–∞–∂–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–≤—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)
            sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)
            sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)
            gradient_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
            gradient_variance = np.var(gradient_magnitude)

            # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –∫–∞–ø—á–∞ –æ–±—ã—á–Ω–æ –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –≥—Ä–∞–Ω–∏—Ü –∏ –≤–∞—Ä–∏–∞—Ü–∏—é –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            if edge_density > 0.15 and gradient_variance > 2000:
                return True

        except Exception:
            pass

        return False

    def _extract_text_with_ocr(self, image) -> str:
        try:
            import easyocr
            try:
                reader = easyocr.Reader(['ru', 'en'], gpu=True)
            except Exception as gpu_error:
                logger.warning(
                    f"‚ö†Ô∏è EasyOCR –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å GPU: {gpu_error}. –ü—Ä–æ–±—É—é CPU...")
                reader = easyocr.Reader(['ru', 'en'], gpu=False)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            result = reader.readtext(image_rgb, detail=0, paragraph=True)
            return "\n".join(result)
        except ImportError:
            logger.warning("‚ö†Ô∏è easyocr –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, OCR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return ""
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ OCR: {str(e)}")
            return ""

    def _analyze_page_text(self, page_text: str, address: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            analysis = []

            # –ü–æ–¥—Å—á–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            result_patterns = [
                r'–Ω–∞–π–¥–µ–Ω–æ[:\s]*(\d+)',
                r'–ø–æ–∫–∞–∑–∞–Ω–æ[:\s]*(\d+)',
                r'—Ä–µ–∑—É–ª—å—Ç–∞—Ç[–æ–≤]*[:\s]*(\d+)',
                r'(\d+)[^\d]*—Ä–µ–∑—É–ª—å—Ç–∞—Ç'
            ]

            for pattern in result_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    analysis.append(
                        f"üìä –ù–∞–π–¥–µ–Ω–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {matches[0]}")
                    break

            # –ü–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            if '–Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ' in page_text.lower():
                analysis.append("‚ùå –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ–æ–±—â–∞–µ—Ç: –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            elif any(word in page_text.lower() for word in ['–∫–∞—Ä—Ç', '–∞–¥—Ä–µ—Å', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ']):
                analysis.append("üó∫Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–∞—Ä—Ç/–∞–¥—Ä–µ—Å–æ–≤")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—à–∏–±–∫–∏
            if any(word in page_text.lower() for word in ['–æ—à–∏–±–∫–∞', 'error', '–ø—Ä–æ–±–ª–µ–º–∞']):
                analysis.append("‚ö†Ô∏è –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ—à–∏–±–æ–∫")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–ø—á—É
            if any(word in page_text.lower() for word in ['–∫–∞–ø—á–∞', 'captcha', '–ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏']):
                analysis.append(
                    "üõ°Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")

            # –ê–Ω–∞–ª–∏–∑ –∞–¥—Ä–µ—Å–∞ –≤ —Ç–µ–∫—Å—Ç–µ
            address_words = address.lower().split()
            found_words = sum(1 for word in address_words if len(
                word) > 3 and word in page_text.lower())
            if found_words > len(address_words) * 0.5:
                analysis.append(
                    f"‚úÖ –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–ª–æ–≤ –∞–¥—Ä–µ—Å–∞ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ ({found_words}/{len(address_words)})")

            analysis.append(f"üìù –û–±—â–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {len(page_text)} —Å–∏–º–≤–æ–ª–æ–≤")

            return "\n".join(analysis)

        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞: {str(e)}"

    def _transliterate_russian(self, text: str) -> str:
        """–¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è —Ä—É—Å—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ"""
        transliteration_dict = {
            '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e', '–∂': 'zh', '–∑': 'z', '–∏': 'i',
            '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm', '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't',
            '—É': 'u', '—Ñ': 'f', '—Ö': 'kh', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'shch', '—ä': '', '—ã': 'y',
            '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya', '–ê': 'A', '–ë': 'B', '–í': 'V', '–ì': 'G', '–î': 'D', '–ï': 'E',
            '–Å': 'E', '–ñ': 'Zh', '–ó': 'Z', '–ò': 'I', '–ô': 'Y', '–ö': 'K', '–õ': 'L', '–ú': 'M', '–ù': 'N', '–û': 'O',
            '–ü': 'P', '–†': 'R', '–°': 'S', '–¢': 'T', '–£': 'U', '–§': 'F', '–•': 'Kh', '–¶': 'Ts', '–ß': 'Ch', '–®': 'Sh',
            '–©': 'Shch', '–™': '', '–´': 'Y', '–¨': '', '–≠': 'E', '–Æ': 'Yu', '–Ø': 'Ya'
        }
        return ''.join(transliteration_dict.get(char, char) for char in text)


def run_local_browser_search(addresses: List[str], headless: bool = True, progress_callback=None) -> List[Dict]:
    """–ó–∞–ø—É—Å–∫ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –±—Ä–∞—É–∑–µ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (Selenium)"""
    import random
    agent = LocalBrowserAgent(headless=headless)
    try:
        agent.open()
        results = []
        for idx, address in enumerate(addresses):
            if progress_callback:
                progress_callback(idx, len(addresses), address)
            logger.info(f"üîç –ü–æ–∏—Å–∫ {idx + 1}/{len(addresses)}: {address}")
            result = agent.search_address_in_yandex(address)
            results.append(result)
            if idx < len(addresses) - 1:
                time.sleep(random.randint(3, 6))
        return results
    finally:
        agent.close()
